{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc32487-ebfd-4a3b-8c77-91a2048e0a17",
   "metadata": {},
   "source": [
    "<center><h1>Hemnani_Hitika_NLP_HW3</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44434dff-5fc8-45b5-9250-ac69c22f4b66",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af174873-4600-4912-a4b6-89f4179d406e",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9569c62b-1b34-46b7-8279-214add5aee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baccf04e-3918-45e5-b627-2c5a2b92be15",
   "metadata": {},
   "source": [
    "Read the training data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa86ae43-8c8f-4aa6-8153-e26add752a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    word  pos\n",
       "0      1  Pierre  NNP\n",
       "1      2  Vinken  NNP\n",
       "2      3       ,    ,\n",
       "3      4      61   CD\n",
       "4      5   years  NNS"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_training_data = pd.read_csv('data/train', sep='\\t', header=None, names=['index', 'word', 'pos'])\n",
    "POS_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9fcdc-3ca9-412f-9c6f-2009da826a7d",
   "metadata": {},
   "source": [
    "### Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66ca268-05bc-4e1f-818a-cb84f76ecc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912095\n"
     ]
    }
   ],
   "source": [
    "###Calculating Word Occurrences in training data\n",
    "\n",
    "vocab_count = POS_training_data['word'].value_counts().reset_index() ### to resolve a warning I had to apply reset_index()\n",
    "vocab_count.columns = ['vocab', 'occurrences']\n",
    "print(len(POS_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475863f4-5d9a-4702-91c1-ed2084a77bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size of the Vocabulary is 43192 with Threshold: 3\n"
     ]
    }
   ],
   "source": [
    "##handling unknown words\n",
    "\n",
    "##Setting Threshold as 3 for now\n",
    "\n",
    "thresh=3\n",
    "###Using a Lambda function took a long time to compute so I searched for a faster why\n",
    "vocab_count['vocab'] = vocab_count['vocab'].where(vocab_count['occurrences'] >= thresh, '<unk>') ###ref:https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.where.html\n",
    "print(f\"Total Size of the Vocabulary is {len(vocab_count)} with Threshold: {thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335d0472-ad88-45a5-8b24-763eecd6c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Occurrences of the special token ‘< unk >’ after replacement is :26274\n"
     ]
    }
   ],
   "source": [
    "Unknown_Count= (vocab_count['vocab'] == '<unk>').sum()\n",
    "print(f\"The total Occurrences of the special token ‘< unk >’ after replacement is :{Unknown_Count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9750f1c-04db-4472-abde-64db37ead208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>occurrences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>46476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>39533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>37452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>22104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>21305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vocab  occurrences\n",
       "0     ,        46476\n",
       "1   the        39533\n",
       "2     .        37452\n",
       "3    of        22104\n",
       "4    to        21305"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting by occurrences in descending order\n",
    "vocab_count = vocab_count.sort_values(by='occurrences', ascending=False)\n",
    "vocab_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2a5ff4-6dfb-491c-908b-ddcfe154a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Removing rare words from vocabulary as we only need one <unk> tag\n",
    "vocab_count=vocab_count[vocab_count['vocab'] !='<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5e87aa-b49f-4dc8-859d-104a6d264de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###adding <unk> as a single row with occurences  \n",
    "vocab_count = pd.concat([pd.DataFrame({'vocab': ['<unk>'], 'occurrences': [Unknown_Count]}), vocab_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48e7a6e6-e0d3-4ba8-b880-2d562d909c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab_index</th>\n",
       "      <th>vocab</th>\n",
       "      <th>occurrences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>26274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>46476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>39533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "      <td>37452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>of</td>\n",
       "      <td>22104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vocab_index  vocab  occurrences\n",
       "0            0  <unk>        26274\n",
       "1            1      ,        46476\n",
       "2            2    the        39533\n",
       "3            3      .        37452\n",
       "4            4     of        22104"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##next step we had is assigning index \n",
    "##we already have an index ...dropping that first and assigning new\n",
    "vocab_count = vocab_count.reset_index(drop=True)\n",
    "vocab_count = vocab_count.reset_index()\n",
    "##Adding it as another column as we need in vocab file\n",
    "vocab_count = vocab_count.rename(columns={'index': 'vocab_index'}) \n",
    "vocab_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "183175c1-808a-49d2-b68f-47511506d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving this Vocab_count dataframe to CSV ref:https://www.datacamp.com/tutorial/save-as-csv-pandas-dataframe?utm_source=google&utm_medium=paid_search&utm_campaignid=19589720830&utm_adgroupid=157098107935&utm_device=c&utm_keyword=&utm_matchtype=&utm_network=g&utm_adpostion=&utm_creative=684592141898&utm_targetid=aud-1645446892440:dsa-2264919291789&utm_loc_interest_ms=&utm_loc_physical_ms=9030933&utm_content=&utm_campaign=230119_1-sea~dsa~tofu_2-b2c_3-nam_4-prc_5-na_6-na_7-le_8-pdsh-go_9-nb-e_10-na_11-na-aifawfeb25&gad_source=1&gclid=CjwKCAiA2cu9BhBhEiwAft6IxGgLiXwkQUf1XpnGLlTcUHhEmyCuNp5fN-DTT5xOeN8niSbJ9qtDwBoCw_oQAvD_BwE\n",
    "##As the 1st line should be <unk> we have to put header=False\n",
    "vocab_count.to_csv('vocab.txt', sep='\\t', columns=['vocab', 'vocab_index', 'occurrences'], index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e100e09-1b1b-4c2b-94ee-447d903e5262",
   "metadata": {},
   "source": [
    "### Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97f1be45-25fb-4571-9619-47738afa2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Here as our vocabulary does not have rare words ... as far as I know we should remove those words from training data as well\n",
    "thresh = 3 \n",
    "##POS_training_data['word'] = POS_training_data['word'].apply(lambda x: x if x in vocab_count[vocab_count['occurrences'] >= thresh]['vocab'].values else '<unk>')\n",
    "##aabove function is taking a lot of time to run so to optimize it I will use the method I used in vocabulary\n",
    "##set of words above threshold\n",
    "valid_vocab = vocab_count[vocab_count['occurrences'] >= thresh]['vocab'].values\n",
    "\n",
    "# Step 2: Replace words in POS_training_data with '<unk>' if not in valid vocab\n",
    "POS_training_data['word'] = POS_training_data['word'].where(POS_training_data['word'].isin(valid_vocab), '<unk>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e4b4737-c91d-4a52-8b13-a04f66b9e91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    word  pos\n",
       "0      1  Pierre  NNP\n",
       "1      2   <unk>  NNP\n",
       "2      3       ,    ,\n",
       "3      4      61   CD\n",
       "4      5   years  NNS"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeadc98c-aad6-47e3-894f-7e10c72c081d",
   "metadata": {},
   "source": [
    "## HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d82808-1dd5-462b-8f45-e74db1cc7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "##seperating words and POS Togs\n",
    "word = POS_training_data['word'].tolist()\n",
    "pos = POS_training_data['pos'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c033049c-b86e-4092-b781-db8f4e8557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word = set(word)\n",
    "unique_pos = set(pos)\n",
    "##we want to work with numbers to help in working with numbers instead of text.\n",
    "num_word = {w: i for i, w in enumerate(unique_word)}\n",
    "num_pos = {p: i for i, p in enumerate(unique_pos)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88887d5c-3cd4-4a72-8dc5-9196b2a2ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creating Dictionary for Transition parameter and Emision parameter\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "# ##Counting Occurences and transitions\n",
    "# tparams_counts = defaultdict(lambda: defaultdict(int)) \n",
    "# pos_counts = defaultdict(int) \n",
    "\n",
    "# ##previous pos_tag and currect pos tag ...how ofter one appears after another\n",
    "# for i in range(1, len(pos)):\n",
    "#     prev_pos = pos[i - 1]\n",
    "#     curr_pos = pos[i]\n",
    "\n",
    "#     tparams_counts[prev_pos][curr_pos] += 1  \n",
    "#     pos_counts[prev_pos] += 1 \n",
    "\n",
    "# ##computing Probability on the basis of count\n",
    "# tparams_probs = defaultdict(dict)\n",
    "\n",
    "# for prev_pos, next_pos in tparams_counts.items():\n",
    "#     total_count = pos_counts[prev_pos] \n",
    "    \n",
    "#     for next_pos, count in next_pos.items():\n",
    "#         tparams_probs[prev_pos][next_pos] = count / total_count \n",
    "\n",
    "\n",
    "##I thought of a new simpler solution \n",
    "tparams_counts = Counter(zip(pos[:-1], pos[1:]))\n",
    "\n",
    "# Count total occurrences of each POS tag as a previous tag\n",
    "pos_counts = Counter(pos)\n",
    "\n",
    "# Compute transition probabilities: P(current_tag | previous_tag)\n",
    "tparams_probs = defaultdict(dict)\n",
    "\n",
    "for (prev_pos, curr_pos), count in tparams_counts.items():\n",
    "    tparams_probs[prev_pos][curr_pos] = count / pos_counts[prev_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e883dde2-4807-4cf9-b01b-b23b5ffddcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Emission Probability\n",
    "# Counting how many times each (POS tag, word) appears and Total Occurences of each tag  \n",
    "eparams_counts = Counter(zip(pos, word))\n",
    "pos_counts = Counter(pos)\n",
    "\n",
    "##computing probability on the basis of count\n",
    "eparams_probs = defaultdict(dict)\n",
    "\n",
    "for (p, w), count in eparams_counts.items():\n",
    "    eparams_probs[p][w] = count / pos_counts[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d19189d9-670b-4388-bca1-b5410df37ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Transition Probabilities:\", tparams_probs)\n",
    "#print(\"Emission Probabilities:\", eparams_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6962fe31-aee2-4599-8f76-c57f3bf89739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(POS_training_data['index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "463af1ac-18be-4a6a-b104-ec0b5dfdc059",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Computing Probility of Initial word\n",
    "initial_counts = Counter()\n",
    "\n",
    "# Counting which POS tags comes in the start of sentence\n",
    "# for data in POS_training_data:\n",
    "#     if data['index'].astype('int') == 1:\n",
    "#         initial_counts[data['pos']] += 1\n",
    "for index, row in POS_training_data.iterrows():\n",
    "    if row['index'] == 1:\n",
    "        initial_counts[row['pos']] += 1\n",
    "\n",
    "##computing probability on the basis of count\n",
    "total_starts = sum(initial_counts.values())  # Total number of first words in all sentences\n",
    "initial_probs = {tag: count / total_starts for tag, count in initial_counts.items()}\n",
    "\n",
    "# Output the initial probabilities\n",
    "#print(\"Initial Probabilities:\", initial_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d453c91-ff58-4f87-95b2-2a8e4cc5eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transition parameters: 1378\n",
      "Number of emission parameters: 23371\n"
     ]
    }
   ],
   "source": [
    "### 4. Computing Number of Parameters\n",
    "num_transition_params = len(tparams_counts)  # Number of unique (s, s') pairs\n",
    "num_emission_params = len(eparams_counts)  # Number of unique (s, x) pairs\n",
    "\n",
    "### Print Results ###\n",
    "print(f\"Number of transition parameters: {num_transition_params}\")\n",
    "print(f\"Number of emission parameters: {num_emission_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1325832-a76a-4c72-88ef-d6a65d1211c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to hmm.json\n"
     ]
    }
   ],
   "source": [
    "##Saving HMM model\n",
    "import json\n",
    "\n",
    "model = {\n",
    "    \"transition\": tparams_probs,  \n",
    "    \"emission\": eparams_probs,    \n",
    "    \"initial\": initial_probs      \n",
    "}\n",
    "\n",
    "with open(\"hmm.json\", \"w\") as f:\n",
    "    json.dump(model, f, indent=4) \n",
    "\n",
    "print(\"Model saved to hmm.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d48738-0627-4eba-9967-dd255e1882b3",
   "metadata": {},
   "source": [
    "### Greedy Decoding with HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39bde280-512d-4899-bc34-596a314a2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.read_csv('data/dev', sep='\\t', header=None)\n",
    "dev_data.columns = ['index', 'word', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c21b1fe3-60ec-43cc-999c-051d169c91e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Corporations</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Commission</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>authorized</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index          word  pos\n",
       "0      1           The   DT\n",
       "1      2       Arizona  NNP\n",
       "2      3  Corporations  NNP\n",
       "3      4    Commission  NNP\n",
       "4      5    authorized  VBD"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10c70f32-eb2a-4c9f-a3e4-bdfc9f16cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The development data is a sequence of words with their POS tags. We need to group the data into sentences. A new sentence starts when the index column is 1\n",
    "###The previous sentence ends when a new sentence starts (i.e., when we encounter the next row with an index == 1)\n",
    "dev_sentences = []\n",
    "current_sentence = []\n",
    "\n",
    "for _, row in dev_data.iterrows():\n",
    "    if row['index'] == 1 and current_sentence:\n",
    "        dev_sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    current_sentence.append(row)\n",
    "\n",
    "dev_sentences.append(current_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ae07056-f6a5-4f63-a472-9a07983c0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c748e500-22ee-4563-8db6-b1be6a1ff2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Applying Greedy Algorithm\n",
    "\n",
    "dev_predictions = []\n",
    "# For each sentence the words are extracted\n",
    "for sentence in dev_sentences:\n",
    "    words = [row['word'] for row in sentence]\n",
    "    predicted_pos = []\n",
    "\n",
    "    for word in words:\n",
    "        best_pos = 'NN'  # Default POS tag if word is unknown\n",
    "        best_prob = 0\n",
    "\n",
    "        # For each word we search through all possible POS tags and pick the one with the highest emission probability\n",
    "        for pos in eparams_probs:\n",
    "            if word in eparams_probs[pos]:\n",
    "                prob = eparams_probs[pos][word]\n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_pos = pos\n",
    "\n",
    "        predicted_pos.append(best_pos)\n",
    "\n",
    "    # The predictions are stored in the same format as the original input data with the word and the predicted POS tag\n",
    "    for i, row in enumerate(sentence):\n",
    "        dev_predictions.append((row['index'], row['word'], predicted_pos[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e909505-f099-4671-9ce1-ac84c6d42c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development predictions saved to greedy_dev.out\n"
     ]
    }
   ],
   "source": [
    "# ##ref:https://www.w3schools.com/python/python_file_write.asp\n",
    "with open('greedy_dev.out', 'w') as f:\n",
    "    for pred in dev_predictions:\n",
    "        f.write(f\"{pred[0]}\\t{pred[1]}\\t{pred[2]}\\n\")\n",
    "\n",
    "print(\"Development predictions saved to greedy_dev.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e26a3f2-8b02-4e08-99fe-d604c452ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "###comploted running eval.py on test data \n",
    "###output:total: 131768, correct: 114962, accuracy: 87.25%\n",
    "\n",
    "test_data = pd.read_csv('data/test', sep='\\t', header=None, names=['index', 'word', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08f08bb4-7af2-4757-8b56-e639114176a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grouping the test data into sentences like we did before for training data\n",
    "test_sentences = []\n",
    "current_sentence = []\n",
    "\n",
    "for _, row in test_data.iterrows():  # Fix: Use test_data instead of dev_data\n",
    "    if row['index'] == 1 and current_sentence:\n",
    "        test_sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    current_sentence.append(row)\n",
    "\n",
    "test_sentences.append(current_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f34c2c7-a17f-4492-87c4-ebaacb464ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Performing Greedy algorithm similarly for test data\n",
    "test_predictions = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    words = [row['word'] for row in sentence]\n",
    "    predicted_pos = []\n",
    "\n",
    "    for word in words:\n",
    "        best_pos = 'NN'  # Default POS tag if word is unknown\n",
    "        best_prob = 0\n",
    "\n",
    "        # Find the POS tag with the highest emission probability for the current word\n",
    "        for pos in eparams_probs:\n",
    "            if word in eparams_probs[pos]:\n",
    "                prob = eparams_probs[pos][word]\n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_pos = pos\n",
    "\n",
    "        predicted_pos.append(best_pos)\n",
    "\n",
    "    for i, row in enumerate(sentence):\n",
    "        test_predictions.append((row['index'], row['word'], predicted_pos[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee5b7292-66f3-4cb0-96f3-434203e9ebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to greedy.out\n"
     ]
    }
   ],
   "source": [
    "with open('greedy.out', 'w') as f:\n",
    "    for pred in test_predictions:\n",
    "        f.write(f\"{pred[0]}\\t{pred[1]}\\t{pred[2]}\\n\")\n",
    "\n",
    "print(\"Predictions saved to greedy.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78f79c-4a94-4dfe-a6cf-843b24936f94",
   "metadata": {},
   "source": [
    "### Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d87611d-dff7-490a-9867-552d73dcbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding(sentence, transition_paramter, emission_paramter, initial_paramter, tags):\n",
    "    num_words = len(sentence)\n",
    "    num_tags = len(tags)\n",
    "\n",
    "    viterbi = [{} for _ in range(num_words)]  \n",
    "    backpointer = [{} for _ in range(num_words)]  # for previous tag\n",
    "    # Smoothing value for unknown words, initial probabilities, and transitions to increase accuracy\n",
    "    # I got 47% accuracy without using smoothing as it fails to take unknown values and make prob 0\n",
    "    smoothing_value = 1e-10\n",
    "    #  for first word using initial_probability\n",
    "    first_word = sentence[0]\n",
    "    for tag in tags:\n",
    "        initial_prob = initial_paramter.get(tag, smoothing_value)\n",
    "        if first_word in emission_paramter[tag]:\n",
    "            emission_prob = emission_paramter[tag][first_word]\n",
    "        else:\n",
    "            emission_prob = smoothing_value\n",
    "\n",
    "        viterbi[0][tag] = initial_prob * emission_prob\n",
    "        backpointer[0][tag] = None  # first tag would not have previous tag\n",
    "\n",
    "    # from next word\n",
    "    for t in range(1, num_words):\n",
    "        word = sentence[t]\n",
    "        for current_tag in tags:\n",
    "            max_prob = -1\n",
    "            best_prev_tag = None\n",
    "\n",
    "            for prev_tag in tags:\n",
    "                if word in emission_paramter[current_tag]:\n",
    "                    emission_prob = emission_paramter[current_tag][word]\n",
    "                else:\n",
    "                    emission_prob = smoothing_value\n",
    "                ###Using Formula viterbi[t][current_tag]=  max of previous tag (viterbi[t−1][prev_tag]×t(current_tag∣prev_tag)×e(word∣current_tag))\n",
    "                ###ref for this formula:https://www.geeksforgeeks.org/viterbi-algorithm-for-hidden-markov-models-hmms/\n",
    "                transition_prob = transition_paramter.get(prev_tag, {}).get(current_tag, smoothing_value)\n",
    "\n",
    "                # Calculate the probability\n",
    "                prob = viterbi[t - 1][prev_tag] * transition_prob * emission_prob\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_prev_tag = prev_tag\n",
    "\n",
    "            viterbi[t][current_tag] = max_prob\n",
    "            backpointer[t][current_tag] = best_prev_tag\n",
    "\n",
    "    # Maximum Probability Selected \n",
    "    best_last_tag = max(viterbi[-1], key=viterbi[-1].get)\n",
    "    best_path = [best_last_tag]\n",
    "    \n",
    "    for t in range(num_words - 1, 0, -1):\n",
    "        best_last_tag = backpointer[t][best_last_tag]\n",
    "        best_path.insert(0, best_last_tag)\n",
    "\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9867410f-2506-411b-873c-76cb4fa56e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "##same as greedy...\n",
    "tags = list(eparams_probs.keys())\n",
    "\n",
    "viterbi_dev_predictions = []\n",
    "\n",
    "for sentence in dev_sentences:\n",
    "    words = [row['word'] for row in sentence]\n",
    "    predicted_tags = viterbi_decoding(words, tparams_probs, eparams_probs, initial_probs, tags)\n",
    "\n",
    "    for i, row in enumerate(sentence):\n",
    "        viterbi_dev_predictions.append((row['index'], row['word'], predicted_tags[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe0e2c83-66b5-4753-8795-c93af8a6533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi predictions saved to viterbi_dev.out\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('viterbi_dev.out', 'w') as f:\n",
    "    for pred in viterbi_dev_predictions:\n",
    "        f.write(f\"{pred[0]}\\t{pred[1]}\\t{pred[2]}\\n\")\n",
    "\n",
    "print(\"Viterbi predictions saved to viterbi_dev.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55f97595-1e0d-4cdf-a70d-d89ada775930",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For test data\n",
    "test_viterbi_predictions = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    words = [row['word'] for row in sentence]\n",
    "    predicted_tags = viterbi_decoding(words, tparams_probs, eparams_probs, initial_probs, tags)\n",
    "\n",
    "    for i, row in enumerate(sentence):\n",
    "        test_viterbi_predictions.append((row['index'], row['word'], predicted_tags[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f1f052a-dd01-48e6-96af-c9498b56263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi predictions for test data saved to viterbi.out\n"
     ]
    }
   ],
   "source": [
    "with open('viterbi.out', 'w') as  f:\n",
    "    for pred in test_viterbi_predictions:\n",
    "        f.write(f\"{pred[0]}\\t{pred[1]}\\t{pred[2]}\\n\")\n",
    "\n",
    "print(\"Viterbi predictions for test data saved to viterbi.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70fda6-993e-4717-b000-a016780c87b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1a283-3553-40d5-bb0c-5a6a8b29e33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
